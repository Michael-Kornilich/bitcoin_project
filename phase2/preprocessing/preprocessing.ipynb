{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run init.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bitcoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Idea: use the new price data to extend the current data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Suply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../temp/total-bitcoins.json\") as f:\n",
    "    total_btc = json.load(f)[\"total-bitcoins\"]\n",
    "total_btc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_btc = pd.DataFrame(total_btc)\n",
    "total_btc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_btc = pd.DataFrame({\n",
    "    \"ts\":(total_btc[\"x\"]/10**3).map(datetime.fromtimestamp),\n",
    "    \"n_coins\":total_btc[\"y\"]\n",
    "}).set_index(\"ts\").squeeze()\n",
    "total_btc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_btc = total_btc.resample(\"ME\").last()\n",
    "total_btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(total_btc, \"total_btc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### The Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_data = pd.read_csv(\n",
    "    \"../../temp/Volume_of_Bitcoin_Futures_(BTC_not_USD)_2025_12_18.csv\",\n",
    "    index_col=\"dt\",\n",
    "    parse_dates=True,\n",
    "    date_format=\"%d/%m/%y %H:%M\"\n",
    ")\n",
    "block_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_data = block_data.resample(\"ME\")[\"volume\"].sum()\n",
    "block_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(block_data, \"btc_volume_block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CMEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "* Downloaded data manually\n",
    "* Starts on 2013-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_exists = pd.Series(\n",
    "    False, \n",
    "    index=pd.date_range(start=\"2013-01-01\", end=\"2025-08-01\", freq=\"ME\").strftime(\"%Y%m\") + \".zip\"\n",
    ")\n",
    "file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in file_exists.index.copy():\n",
    "    file_exists.loc[i] = Path(f\"../../temp/{i}\").exists()\n",
    "file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_exists[file_exists == False]\n",
    "# all files are present"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# should only be ran only once, hence raw cell\n",
    "\n",
    "for file in Path(\"../../temp/\").iterdir():\n",
    "    # other files will fail on extraction\n",
    "    if not re.match(r\"^20.{4}\\.zip$\", file.name): # match \"20****.zip\" files\n",
    "        continue\n",
    "    \n",
    "    base_name = file.name.split(\".\")[0]\n",
    "    \n",
    "    with zipfile.ZipFile(file, \"r\") as archive:\n",
    "        try: \n",
    "            archive.extract(f\"Web_Volume_Report_CMEG_{base_name}.pdf\", path=file.parent)\n",
    "            file.unlink()\n",
    "            continue\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            archive.extract(f\"{base_name}/Web_Volume_Report_CMEG_{base_name}.pdf\", path=file.parent)\n",
    "            file.unlink()\n",
    "        except Exception as err:\n",
    "            print(f\"FATAL: While processing, file 'Web_Volume_Report_CMEG_{base_name}.pdf' has not been found, skipping\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "TARGET_DIR = Path(\"/Users/Misha/Documents/python_projects/data_analysis/bitcoin_analysis/temp/\")\n",
    "\n",
    "if not TARGET_DIR.is_dir():\n",
    "    raise ValueError(\"TARGET_DIR must be an existing directory\")\n",
    "\n",
    "for item in list(TARGET_DIR.iterdir()):\n",
    "    if not item.is_dir():\n",
    "        continue\n",
    "        \n",
    "    for sub_item in item.iterdir():\n",
    "        destination = TARGET_DIR / sub_item.name\n",
    "\n",
    "        if destination.exists():\n",
    "            raise FileExistsError(f\"Name collision: {destination} already exists\")\n",
    "\n",
    "        shutil.move(str(sub_item), str(destination))\n",
    "    item.rmdir()\n",
    "\n",
    "# some achives deviate, thus they cannot be unpacked with these two scripts (cell above and this). They thus require manual extraction. \n",
    "# This is especially true of 2015-01, where the report is mistakenly called ...201412.zip"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# this script should also be ran only once\n",
    "for i in list(Path(\"../../temp/\").iterdir()):\n",
    "    if not i.name.startswith(\"Web\"):\n",
    "        print(f\"Unrecognized object: {i.name}\")\n",
    "        continue\n",
    "    \n",
    "    shutil.move(i, i.parent / i.name[18:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_files = list(\n",
    "    \"CMEG_\" + pd.date_range(start=\"2013-01-01\", end=\"2025-08-01\", freq=\"ME\").strftime(\"%Y%m\") + \".pdf\"\n",
    ")\n",
    "expected_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore dot files\n",
    "got_files = list(i.name for i in Path(\"../../temp/\").iterdir() if not i.name.startswith(\".\"))\n",
    "got_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(expected_files) == len(got_files), f\"The number of actual files does not equal what is expected. Expected: {len(expected_files)}, got {len(got_files)}\"\n",
    "\n",
    "assert len(got_files) == len(set(got_files)), \"There are duplicated in fetched files\"\n",
    "\n",
    "for exp, got in zip(sorted(expected_files), sorted(got_files)):\n",
    "    assert exp == got, f\"File names don't match: {exp} vs. {got}\"\n",
    "else:\n",
    "    print(f\"All expected files have been fetched and processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = pd.Series()\n",
    "\n",
    "for file in Path(\"../../temp/\").iterdir():\n",
    "    if not file.name.startswith(\"CMEG\"):\n",
    "        continue\n",
    "\n",
    "    for name in [\"BITCOIN FUTURES\", \"BITCOIN\"]:            \n",
    "        try:\n",
    "            volume = extract_asset_volume(\n",
    "                path=file,\n",
    "                asset=name\n",
    "            )\n",
    "            break\n",
    "        except ValueError:\n",
    "            volume = -1\n",
    "    srs.loc[datetime(int(file.name[5:9]), int(file.name[9:11]), 1)] = volume\n",
    "    print(f\"{datetime(int(file.name[5:9]), int(file.name[9:11]), 1)}: {volume}\")\n",
    "\n",
    "srs = srs.sort_index()\n",
    "srs[srs == -1] = np.nan\n",
    "srs = srs * 5\n",
    "srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(srs, \"CMEG_volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_volume = load_pickle(\"btc_volume_block\")\n",
    "block_volume.index.name = \"\"\n",
    "block_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmeg_volume = load_pickle(\"CMEG_volume\")\n",
    "cmeg_volume = cmeg_volume.resample(\"ME\").last()\n",
    "cmeg_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_volume = (block_volume + cmeg_volume).dropna()\n",
    "total_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_btc = load_pickle(\"total_btc\")\n",
    "total_btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [total_btc, total_volume],\n",
    "    axis=\"columns\",\n",
    "    join=\"outer\"\n",
    ")\n",
    "df = df.rename(columns={\"n_coins\":\"supply\", 0:\"trading_volume\"})\n",
    "df = df.reindex(\n",
    "    pd.date_range(\"2010-01-01\", \"2025-08-01\", freq=\"ME\")\n",
    ")\n",
    "df = df.dropna(axis=\"rows\", how=\"any\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df[\"index\"] = \"'\" + df[\"index\"].dt.strftime(\"%F\") + \"'\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_insert_into_db(\n",
    "    conn_str=CONNECTION_STRING, \n",
    "    table=\"bitcoin_trading_metadata\",\n",
    "    data=df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_db(\n",
    "    conn_str=CONNECTION_STRING, \n",
    "    table=\"bitcoin_trading_metadata\",\n",
    "    data=df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Bitcoin supply and demand have been acquired and inserted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# S&P 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Supply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "* downloaded data manually from the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    io = \"../../temp/snp_500_data.xlsx\",\n",
    "    sheet_name = \"Historisch\",\n",
    "    header = 0,\n",
    "    usecols = [\"per\", \"Umlaufende Anteile\"],\n",
    "    na_values = \"--\",\n",
    "    thousands = \",\",\n",
    "    decimal = \".\"\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split string dates into day, month, year\n",
    "# some have form \"08.Okt.2025\", some are missing a full stop after the month \"14.M채rz2025\"\n",
    "# So we:\n",
    "#     match the day into the first group, \n",
    "#     then skip the point, \n",
    "#     match month, \n",
    "#     optional point, \n",
    "#     match the year\n",
    "pattern = re.compile(r\"(\\d{1,2})\\.(Jan|Feb|M채rz|Apr|Mai|Juni|Juli|Aug|Sept|Okt|Nov|Dez)\\.?(\\d{4})\")\n",
    "month_mapping = {k:v for k, v in zip(\"Jan|Feb|M채rz|Apr|Mai|Juni|Juli|Aug|Sept|Okt|Nov|Dez\".split(\"|\"),range(1,13))}\n",
    "dates = []\n",
    "\n",
    "for i in df[\"per\"]:\n",
    "    match_object = re.match(pattern, i)\n",
    "    group = list(match_object.groups())\n",
    "    group[1] = month_mapping[group[1]]\n",
    "    group = list(map(int, group))\n",
    "    group = group[::-1]\n",
    "\n",
    "    dates.append(\n",
    "        datetime(*group)\n",
    "    )\n",
    "dates[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"per\"] = dates\n",
    "df = df.rename(columns={\"per\":\"dt\", \"Umlaufende Anteile\": \"outstanding_supply\"})\n",
    "df = df.set_index(\"dt\")\n",
    "df = df.squeeze().sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the supply does show significant downward fluctuations.\n",
    "# Thus, monthly average will be taken\n",
    "df.plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\n",
    "    .resample(\"ME\")\n",
    "    .mean()\n",
    "    .loc[\"2010-01-01\":\"2025-08-01\"]\n",
    "    .copy()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(\n",
    "    df,\n",
    "    \"snp_aggregated_supply_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "* Manually save the [source page](https://www.ishares.com/uk/individual/en/products/253743/ishares-sp-500-b-ucits-etf-acc-fund) into temp (HTML only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER_TYPE = \"Bloomberg Ticker\"\n",
    "\n",
    "with open(\"../../temp/iShares Core S&P 500 UCITS ETF _ CSPX.html\") as f:\n",
    "    html_dump = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_dump, \"html.parser\")\n",
    "table = soup.find(\"table\",attrs={\"id\":\"listingsTable\"})\n",
    "df = pd.read_html(\n",
    "    StringIO(str(table)),\n",
    "    na_values=\"-\"\n",
    ")[0]\n",
    "\n",
    "del html_dump, soup, table\n",
    "\n",
    "na_exchanges = list(df.loc[df[TICKER_TYPE].isna(), \"Exchange\"])\n",
    "yf_tickers = list(df[TICKER_TYPE].dropna())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf_data = yf.download(\n",
    "    tickers=list(df[\"RIC\"].dropna()) + [\"CSPX.AS\", \"CSSPX.SW\"],\n",
    "    interval=\"1mo\",\n",
    "    start=\"2010-01-01\",\n",
    "    end=\"2025-08-01\",\n",
    "    group_by=\"Ticker\",\n",
    "    keepna=True\n",
    ")\n",
    "\n",
    "yf_data = yf_data.xs(\n",
    "    \"Volume\", \n",
    "    axis=\"columns\", \n",
    "    level=1\n",
    ")\n",
    "\n",
    "trading_volume = yf_data.dropna(how=\"all\").sum(axis=\"columns\") / 0.93\n",
    "trading_volume = trading_volume.resample(\"ME\").last()\n",
    "trading_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Missing exchanges:\n",
    "* Bolsa De Valores De Colombia\n",
    "* Santiago Stock Exchange\n",
    "* Tel Aviv Stock Exchange\n",
    "\n",
    "Together ~7% of the listed exchanges. Source: ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(trading_volume, \"snp_trading_volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "supply = load_pickle(\"snp_aggregated_supply_data\")\n",
    "volume = load_pickle(\"snp_trading_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [supply, volume],\n",
    "    axis=\"columns\",\n",
    "    join=\"outer\"\n",
    ")\n",
    "df = df.rename(columns={0:\"volume\"})\n",
    "\n",
    "dates = pd.date_range(start=\"2010-01-01\", end=\"2025-08-01\", freq=\"ME\")\n",
    "df.reindex(dates).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate index\n",
    "pd.Series(df.index).is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df[\"index\"] = \"'\" + df[\"index\"].astype(str) + \"'\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_insert_into_db(\n",
    "    CONNECTION_STRING,\n",
    "    \"snp_trading_metadata\",\n",
    "    df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_db(\n",
    "    CONNECTION_STRING,\n",
    "    \"snp_trading_metadata\",\n",
    "    df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"S&P 500 supply and demand have been acquired and inserted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NASDAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Supply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "* downloaded data manually from the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    io = \"../../temp/nasdaq_data.xlsx\",\n",
    "    sheet_name = \"Historical\",\n",
    "    header = 0,\n",
    "    usecols = [\"As Of\", \"Securities In Issue\"],\n",
    "    na_values = \"--\",\n",
    "    thousands = \",\",\n",
    "    decimal = \".\"\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split string dates into day, month, year\n",
    "# some have form \"08.Okt.2025\", some are missing a full stop after the month \"14.M채rz2025\"\n",
    "# So we:\n",
    "#     match the day into the first group, \n",
    "#     then skip the point, \n",
    "#     match month, \n",
    "#     optional point, \n",
    "#     match the year\n",
    "pattern = re.compile(r\"(\\d{1,2})\\/(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sept|Oct|Nov|Dec)\\/?(\\d{4})\")\n",
    "month_mapping = {k:v for k, v in zip(\"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sept|Oct|Nov|Dec\".split(\"|\"),range(1,13))}\n",
    "dates = []\n",
    "\n",
    "for i in df[\"As Of\"]:\n",
    "    match_object = re.match(pattern, i)\n",
    "    group = list(match_object.groups())\n",
    "    group[1] = month_mapping[group[1]]\n",
    "    group = list(map(int, group))\n",
    "    group = group[::-1]\n",
    "\n",
    "    dates.append(\n",
    "        datetime(*group)\n",
    "    )\n",
    "dates[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"As Of\"] = dates\n",
    "df = df.rename(columns={\"As Of\":\"dt\", \"Securities In Issue\": \"outstanding_supply\"})\n",
    "df = df.set_index(\"dt\")\n",
    "df = df.squeeze().sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the supply does show significant downward fluctuations.\n",
    "# Thus, monthly average will be taken\n",
    "df.plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\n",
    "    .resample(\"ME\")\n",
    "    .mean()\n",
    "    .loc[\"2010-01-01\":\"2025-08-01\"]\n",
    "    .copy()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(\n",
    "    df,\n",
    "    \"nasdaq_aggregated_supply_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER_TYPE = \"Bloomberg Ticker\"\n",
    "\n",
    "with open(\"../../temp/iShares NASDAQ 100 UCITS ETF _ CNDX.html\") as f:\n",
    "    html_dump = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_dump, \"html.parser\")\n",
    "table = soup.find(\"table\",attrs={\"id\":\"listingsTable\"})\n",
    "df = pd.read_html(\n",
    "    StringIO(str(table)),\n",
    "    na_values=\"-\"\n",
    ")[0]\n",
    "\n",
    "del html_dump, soup, table\n",
    "\n",
    "na_exchanges = list(df.loc[df[TICKER_TYPE].isna(), \"Exchange\"])\n",
    "yf_tickers = list(df[TICKER_TYPE].dropna())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "* Tel Aviv is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf_data = yf.download(\n",
    "    tickers=list(df[\"RIC\"].dropna()) + [\"CNDX.AS\", \"CSNDX.SW\"],\n",
    "    interval=\"1mo\",\n",
    "    start=\"2010-01-01\",\n",
    "    end=\"2025-08-01\",\n",
    "    group_by=\"Ticker\",\n",
    "    keepna=True\n",
    ")\n",
    "\n",
    "yf_data = yf_data.xs(\n",
    "    \"Volume\", \n",
    "    axis=\"columns\", \n",
    "    level=1\n",
    ")\n",
    "\n",
    "trading_volume = yf_data.dropna(how=\"all\").sum(axis=\"columns\") / 0.93\n",
    "trading_volume = trading_volume.resample(\"ME\").last()\n",
    "trading_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(\n",
    "    trading_volume,\n",
    "    \"nasdaq_trading_volume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "supply = load_pickle(\"nasdaq_aggregated_supply_data\")\n",
    "trading_volume = load_pickle(\"nasdaq_trading_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [supply, trading_volume],\n",
    "    axis=\"columns\",\n",
    "    join=\"outer\"\n",
    ")\n",
    "df = df.rename(columns={0:\"trading_volume\"})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df[\"index\"] = \"'\" + df[\"index\"].astype(\"str\") + \"'\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_insert_into_db(\n",
    "    CONNECTION_STRING,\n",
    "    \"nasdaq_trading_metadata\",\n",
    "    df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_db(\n",
    "    CONNECTION_STRING,\n",
    "    \"nasdaq_trading_metadata\",\n",
    "    df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Successfully inserted NASDAQ trading metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "# Dow Jones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
