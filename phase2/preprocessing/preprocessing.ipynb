{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run init.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Bitcoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Idea: use the new price data to extend the current data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Suply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../temp/total-bitcoins.json\") as f:\n",
    "    total_btc = json.load(f)[\"total-bitcoins\"]\n",
    "total_btc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_btc = pd.DataFrame(total_btc)\n",
    "total_btc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_btc = pd.DataFrame({\n",
    "    \"ts\":(total_btc[\"x\"]/10**3).map(datetime.fromtimestamp),\n",
    "    \"n_coins\":total_btc[\"y\"]\n",
    "}).set_index(\"ts\").squeeze()\n",
    "total_btc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_btc = total_btc.resample(\"ME\").last()\n",
    "total_btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(total_btc, \"total_btc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### The Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_data = pd.read_csv(\n",
    "    \"../../temp/Volume_of_Bitcoin_Futures_(BTC_not_USD)_2025_12_18.csv\",\n",
    "    index_col=\"dt\",\n",
    "    parse_dates=True,\n",
    "    date_format=\"%d/%m/%y %H:%M\"\n",
    ")\n",
    "block_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_data = block_data.resample(\"ME\")[\"volume\"].sum()\n",
    "block_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(block_data, \"btc_volume_block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CMEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "* Downloaded data manually\n",
    "* Starts on 2013-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_exists = pd.Series(\n",
    "    False, \n",
    "    index=pd.date_range(start=\"2013-01-01\", end=\"2025-08-01\", freq=\"ME\").strftime(\"%Y%m\") + \".zip\"\n",
    ")\n",
    "file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in file_exists.index.copy():\n",
    "    file_exists.loc[i] = Path(f\"../../temp/{i}\").exists()\n",
    "file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_exists[file_exists == False]\n",
    "# all files are present"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# should only be ran only once, hence raw cell\n",
    "\n",
    "for file in Path(\"../../temp/\").iterdir():\n",
    "    # other files will fail on extraction\n",
    "    if not re.match(r\"^20.{4}\\.zip$\", file.name): # match \"20****.zip\" files\n",
    "        continue\n",
    "    \n",
    "    base_name = file.name.split(\".\")[0]\n",
    "    \n",
    "    with zipfile.ZipFile(file, \"r\") as archive:\n",
    "        try: \n",
    "            archive.extract(f\"Web_Volume_Report_CMEG_{base_name}.pdf\", path=file.parent)\n",
    "            file.unlink()\n",
    "            continue\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            archive.extract(f\"{base_name}/Web_Volume_Report_CMEG_{base_name}.pdf\", path=file.parent)\n",
    "            file.unlink()\n",
    "        except Exception as err:\n",
    "            print(f\"FATAL: While processing, file 'Web_Volume_Report_CMEG_{base_name}.pdf' has not been found, skipping\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "TARGET_DIR = Path(\"/Users/Misha/Documents/python_projects/data_analysis/bitcoin_analysis/temp/\")\n",
    "\n",
    "if not TARGET_DIR.is_dir():\n",
    "    raise ValueError(\"TARGET_DIR must be an existing directory\")\n",
    "\n",
    "for item in list(TARGET_DIR.iterdir()):\n",
    "    if not item.is_dir():\n",
    "        continue\n",
    "        \n",
    "    for sub_item in item.iterdir():\n",
    "        destination = TARGET_DIR / sub_item.name\n",
    "\n",
    "        if destination.exists():\n",
    "            raise FileExistsError(f\"Name collision: {destination} already exists\")\n",
    "\n",
    "        shutil.move(str(sub_item), str(destination))\n",
    "    item.rmdir()\n",
    "\n",
    "# some achives deviate, thus they cannot be unpacked with these two scripts (cell above and this). They thus require manual extraction. \n",
    "# This is especially true of 2015-01, where the report is mistakenly called ...201412.zip"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# this script should also be ran only once\n",
    "for i in list(Path(\"../../temp/\").iterdir()):\n",
    "    if not i.name.startswith(\"Web\"):\n",
    "        print(f\"Unrecognized object: {i.name}\")\n",
    "        continue\n",
    "    \n",
    "    shutil.move(i, i.parent / i.name[18:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_files = list(\n",
    "    \"CMEG_\" + pd.date_range(start=\"2013-01-01\", end=\"2025-08-01\", freq=\"ME\").strftime(\"%Y%m\") + \".pdf\"\n",
    ")\n",
    "expected_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore dot files\n",
    "got_files = list(i.name for i in Path(\"../../temp/\").iterdir() if not i.name.startswith(\".\"))\n",
    "got_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(expected_files) == len(got_files), f\"The number of actual files does not equal what is expected. Expected: {len(expected_files)}, got {len(got_files)}\"\n",
    "\n",
    "assert len(got_files) == len(set(got_files)), \"There are duplicated in fetched files\"\n",
    "\n",
    "for exp, got in zip(sorted(expected_files), sorted(got_files)):\n",
    "    assert exp == got, f\"File names don't match: {exp} vs. {got}\"\n",
    "else:\n",
    "    print(f\"All expected files have been fetched and processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = pd.Series()\n",
    "\n",
    "for file in Path(\"../../temp/\").iterdir():\n",
    "    if not file.name.startswith(\"CMEG\"):\n",
    "        continue\n",
    "\n",
    "    for name in [\"BITCOIN FUTURES\", \"BITCOIN\"]:            \n",
    "        try:\n",
    "            volume = extract_asset_volume(\n",
    "                path=file,\n",
    "                asset=name\n",
    "            )\n",
    "            break\n",
    "        except ValueError:\n",
    "            volume = -1\n",
    "    srs.loc[datetime(int(file.name[5:9]), int(file.name[9:11]), 1)] = volume\n",
    "    print(f\"{datetime(int(file.name[5:9]), int(file.name[9:11]), 1)}: {volume}\")\n",
    "\n",
    "srs = srs.sort_index()\n",
    "srs[srs == -1] = np.nan\n",
    "srs = srs * 5\n",
    "srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(srs, \"CMEG_volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_volume = load_pickle(\"btc_volume_block\")\n",
    "block_volume.index.name = \"\"\n",
    "block_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmeg_volume = load_pickle(\"CMEG_volume\")\n",
    "cmeg_volume = cmeg_volume.resample(\"ME\").last()\n",
    "cmeg_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_volume = (block_volume + cmeg_volume).dropna()\n",
    "total_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_btc = load_pickle(\"total_btc\")\n",
    "total_btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [total_btc, total_volume],\n",
    "    axis=\"columns\",\n",
    "    join=\"outer\"\n",
    ")\n",
    "df = df.rename(columns={\"n_coins\":\"supply\", 0:\"trading_volume\"})\n",
    "df = df.reindex(\n",
    "    pd.date_range(\"2010-01-01\", \"2025-08-01\", freq=\"ME\")\n",
    ")\n",
    "df = df.dropna(axis=\"rows\", how=\"any\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df[\"index\"] = \"'\" + df[\"index\"].dt.strftime(\"%F\") + \"'\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_insert_into_db(\n",
    "    conn_str=CONNECTION_STRING, \n",
    "    table=\"bitcoin_trading_metadata\",\n",
    "    data=df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_db(\n",
    "    conn_str=CONNECTION_STRING, \n",
    "    table=\"bitcoin_trading_metadata\",\n",
    "    data=df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Bitcoin supply and demand have been acquired and inserted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# S&P 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Supply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "* downloaded data manually from the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    io = \"../../temp/snp_500_data.xlsx\",\n",
    "    sheet_name = \"Historisch\",\n",
    "    header = 0,\n",
    "    usecols = [\"per\", \"Umlaufende Anteile\"],\n",
    "    na_values = \"--\",\n",
    "    thousands = \",\",\n",
    "    decimal = \".\"\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split string dates into day, month, year\n",
    "# some have form \"08.Okt.2025\", some are missing a full stop after the month \"14.März2025\"\n",
    "# So we:\n",
    "#     match the day into the first group, \n",
    "#     then skip the point, \n",
    "#     match month, \n",
    "#     optional point, \n",
    "#     match the year\n",
    "pattern = re.compile(r\"(\\d{1,2})\\.(Jan|Feb|März|Apr|Mai|Juni|Juli|Aug|Sept|Okt|Nov|Dez)\\.?(\\d{4})\")\n",
    "month_mapping = {k:v for k, v in zip(\"Jan|Feb|März|Apr|Mai|Juni|Juli|Aug|Sept|Okt|Nov|Dez\".split(\"|\"),range(1,13))}\n",
    "dates = []\n",
    "\n",
    "for i in df[\"per\"]:\n",
    "    match_object = re.match(pattern, i)\n",
    "    group = list(match_object.groups())\n",
    "    group[1] = month_mapping[group[1]]\n",
    "    group = list(map(int, group))\n",
    "    group = group[::-1]\n",
    "\n",
    "    dates.append(\n",
    "        datetime(*group)\n",
    "    )\n",
    "dates[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"per\"] = dates\n",
    "df = df.rename(columns={\"per\":\"dt\", \"Umlaufende Anteile\": \"outstanding_supply\"})\n",
    "df = df.set_index(\"dt\")\n",
    "df = df.squeeze().sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the supply does show significant downward fluctuations.\n",
    "# Thus, monthly average will be taken\n",
    "df.plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\n",
    "    .resample(\"ME\")\n",
    "    .mean()\n",
    "    .loc[\"2010-01-01\":\"2025-08-01\"]\n",
    "    .copy()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(\n",
    "    df,\n",
    "    \"snp_aggregated_supply_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "* Manually save the [source page](https://www.ishares.com/uk/individual/en/products/253743/ishares-sp-500-b-ucits-etf-acc-fund) into temp (HTML only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER_TYPE = \"Bloomberg Ticker\"\n",
    "\n",
    "with open(\"../../temp/iShares Core S&P 500 UCITS ETF _ CSPX.html\") as f:\n",
    "    html_dump = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_dump, \"html.parser\")\n",
    "table = soup.find(\"table\",attrs={\"id\":\"listingsTable\"})\n",
    "df = pd.read_html(\n",
    "    StringIO(str(table)),\n",
    "    na_values=\"-\"\n",
    ")[0]\n",
    "\n",
    "del html_dump, soup, table\n",
    "\n",
    "na_exchanges = list(df.loc[df[TICKER_TYPE].isna(), \"Exchange\"])\n",
    "yf_tickers = list(df[TICKER_TYPE].dropna())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf_data = yf.download(\n",
    "    tickers=list(df[\"RIC\"].dropna()) + [\"CSPX.AS\", \"CSSPX.SW\"],\n",
    "    interval=\"1mo\",\n",
    "    start=\"2010-01-01\",\n",
    "    end=\"2025-08-01\",\n",
    "    group_by=\"Ticker\",\n",
    "    keepna=True\n",
    ")\n",
    "\n",
    "yf_data = yf_data.xs(\n",
    "    \"Volume\", \n",
    "    axis=\"columns\", \n",
    "    level=1\n",
    ")\n",
    "\n",
    "trading_volume = yf_data.dropna(how=\"all\").sum(axis=\"columns\") / 0.93\n",
    "trading_volume = trading_volume.resample(\"ME\").last()\n",
    "trading_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Missing exchanges:\n",
    "* Bolsa De Valores De Colombia\n",
    "* Santiago Stock Exchange\n",
    "* Tel Aviv Stock Exchange\n",
    "\n",
    "Together ~7% of the listed exchanges. Source: ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(trading_volume, \"snp_trading_volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "supply = load_pickle(\"snp_aggregated_supply_data\")\n",
    "volume = load_pickle(\"snp_trading_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [supply, volume],\n",
    "    axis=\"columns\",\n",
    "    join=\"outer\"\n",
    ")\n",
    "df = df.rename(columns={0:\"volume\"})\n",
    "\n",
    "dates = pd.date_range(start=\"2010-01-01\", end=\"2025-08-01\", freq=\"ME\")\n",
    "df.reindex(dates).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate index\n",
    "pd.Series(df.index).is_monotonic_increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df[\"index\"] = \"'\" + df[\"index\"].astype(str) + \"'\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_insert_into_db(\n",
    "    CONNECTION_STRING,\n",
    "    \"snp_trading_metadata\",\n",
    "    df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_db(\n",
    "    CONNECTION_STRING,\n",
    "    \"snp_trading_metadata\",\n",
    "    df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"S&P 500 supply and demand have been acquired and inserted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NASDAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Supply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "* downloaded data manually from the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    io = \"../../temp/nasdaq_data.xlsx\",\n",
    "    sheet_name = \"Historical\",\n",
    "    header = 0,\n",
    "    usecols = [\"As Of\", \"Securities In Issue\"],\n",
    "    na_values = \"--\",\n",
    "    thousands = \",\",\n",
    "    decimal = \".\"\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split string dates into day, month, year\n",
    "# some have form \"08.Okt.2025\", some are missing a full stop after the month \"14.März2025\"\n",
    "# So we:\n",
    "#     match the day into the first group, \n",
    "#     then skip the point, \n",
    "#     match month, \n",
    "#     optional point, \n",
    "#     match the year\n",
    "pattern = re.compile(r\"(\\d{1,2})\\/(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sept|Oct|Nov|Dec)\\/?(\\d{4})\")\n",
    "month_mapping = {k:v for k, v in zip(\"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sept|Oct|Nov|Dec\".split(\"|\"),range(1,13))}\n",
    "dates = []\n",
    "\n",
    "for i in df[\"As Of\"]:\n",
    "    match_object = re.match(pattern, i)\n",
    "    group = list(match_object.groups())\n",
    "    group[1] = month_mapping[group[1]]\n",
    "    group = list(map(int, group))\n",
    "    group = group[::-1]\n",
    "\n",
    "    dates.append(\n",
    "        datetime(*group)\n",
    "    )\n",
    "dates[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"As Of\"] = dates\n",
    "df = df.rename(columns={\"As Of\":\"dt\", \"Securities In Issue\": \"outstanding_supply\"})\n",
    "df = df.set_index(\"dt\")\n",
    "df = df.squeeze().sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the supply does show significant downward fluctuations.\n",
    "# Thus, monthly average will be taken\n",
    "df.plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\n",
    "    .resample(\"ME\")\n",
    "    .mean()\n",
    "    .loc[\"2010-01-01\":\"2025-08-01\"]\n",
    "    .copy()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(\n",
    "    df,\n",
    "    \"nasdaq_aggregated_supply_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER_TYPE = \"Bloomberg Ticker\"\n",
    "\n",
    "with open(\"../../temp/iShares NASDAQ 100 UCITS ETF _ CNDX.html\") as f:\n",
    "    html_dump = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_dump, \"html.parser\")\n",
    "table = soup.find(\"table\",attrs={\"id\":\"listingsTable\"})\n",
    "df = pd.read_html(\n",
    "    StringIO(str(table)),\n",
    "    na_values=\"-\"\n",
    ")[0]\n",
    "\n",
    "del html_dump, soup, table\n",
    "\n",
    "na_exchanges = list(df.loc[df[TICKER_TYPE].isna(), \"Exchange\"])\n",
    "yf_tickers = list(df[TICKER_TYPE].dropna())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "* Tel Aviv is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf_data = yf.download(\n",
    "    tickers=list(df[\"RIC\"].dropna()) + [\"CNDX.AS\", \"CSNDX.SW\"],\n",
    "    interval=\"1mo\",\n",
    "    start=\"2010-01-01\",\n",
    "    end=\"2025-08-01\",\n",
    "    group_by=\"Ticker\",\n",
    "    keepna=True\n",
    ")\n",
    "\n",
    "yf_data = yf_data.xs(\n",
    "    \"Volume\", \n",
    "    axis=\"columns\", \n",
    "    level=1\n",
    ")\n",
    "\n",
    "trading_volume = yf_data.dropna(how=\"all\").sum(axis=\"columns\") / 0.93\n",
    "trading_volume = trading_volume.resample(\"ME\").last()\n",
    "trading_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(\n",
    "    trading_volume,\n",
    "    \"nasdaq_trading_volume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "supply = load_pickle(\"nasdaq_aggregated_supply_data\")\n",
    "trading_volume = load_pickle(\"nasdaq_trading_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [supply, trading_volume],\n",
    "    axis=\"columns\",\n",
    "    join=\"outer\"\n",
    ")\n",
    "df = df.rename(columns={0:\"trading_volume\"})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df[\"index\"] = \"'\" + df[\"index\"].astype(\"str\") + \"'\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_insert_into_db(\n",
    "    CONNECTION_STRING,\n",
    "    \"nasdaq_trading_metadata\",\n",
    "    df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_db(\n",
    "    CONNECTION_STRING,\n",
    "    \"nasdaq_trading_metadata\",\n",
    "    df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Successfully inserted NASDAQ trading metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dow Jones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Supply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "* downloaded data manually from the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    io = \"../../temp/dow_jones_data.xlsx\",\n",
    "    sheet_name = \"Historical\",\n",
    "    header = 0,\n",
    "    usecols = [\"As Of\", \"Securities In Issue\"],\n",
    "    na_values = \"--\",\n",
    "    thousands = \",\",\n",
    "    decimal = \".\"\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split string dates into day, month, year\n",
    "# some have form \"08.Okt.2025\", some are missing a full stop after the month \"14.März2025\"\n",
    "# So we:\n",
    "#     match the day into the first group, \n",
    "#     then skip the point, \n",
    "#     match month, \n",
    "#     optional point, \n",
    "#     match the year\n",
    "pattern = re.compile(r\"(\\d{1,2})\\/(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sept|Oct|Nov|Dec)\\/?(\\d{4})\")\n",
    "month_mapping = {k:v for k, v in zip(\"Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sept|Oct|Nov|Dec\".split(\"|\"),range(1,13))}\n",
    "dates = []\n",
    "\n",
    "for i in df[\"As Of\"]:\n",
    "    match_object = re.match(pattern, i)\n",
    "    group = list(match_object.groups())\n",
    "    group[1] = month_mapping[group[1]]\n",
    "    group = list(map(int, group))\n",
    "    group = group[::-1]\n",
    "\n",
    "    dates.append(\n",
    "        datetime(*group)\n",
    "    )\n",
    "dates[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"As Of\"] = dates\n",
    "df = df.rename(columns={\"As Of\":\"dt\", \"Securities In Issue\": \"outstanding_supply\"})\n",
    "df = df.set_index(\"dt\")\n",
    "df = df.squeeze().sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\n",
    "    .resample(\"ME\")\n",
    "    .mean()\n",
    "    .loc[\"2010-01-01\":\"2025-08-01\"]\n",
    "    .copy()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(\n",
    "    df,\n",
    "    \"dow_jones_aggregated_supply_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER_TYPE = \"Bloomberg Ticker\"\n",
    "\n",
    "with open(\"../../temp/iShares Dow Jones Industrial Average UCITS ETF _ CIND.html\") as f:\n",
    "    html_dump = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html_dump, \"html.parser\")\n",
    "table = soup.find(\"table\",attrs={\"id\":\"listingsTable\"})\n",
    "df = pd.read_html(\n",
    "    StringIO(str(table)),\n",
    "    na_values=\"-\"\n",
    ")[0]\n",
    "\n",
    "del html_dump, soup, table\n",
    "\n",
    "na_exchanges = list(df.loc[df[TICKER_TYPE].isna(), \"Exchange\"])\n",
    "yf_tickers = list(df[TICKER_TYPE].dropna())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "yf_data = yf.download(\n",
    "    tickers=list(df[\"RIC\"].dropna()) + [\"CSINDU.SW\"],\n",
    "    interval=\"1mo\",\n",
    "    start=\"2010-01-01\",\n",
    "    end=\"2025-08-01\",\n",
    "    group_by=\"Ticker\",\n",
    "    keepna=True\n",
    ")\n",
    "\n",
    "yf_data = yf_data.xs(\n",
    "    \"Volume\", \n",
    "    axis=\"columns\", \n",
    "    level=1\n",
    ")\n",
    "\n",
    "trading_volume = yf_data.dropna(how=\"all\").sum(axis=\"columns\") / 0.93\n",
    "trading_volume = trading_volume.resample(\"ME\").last()\n",
    "trading_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(\n",
    "    trading_volume,\n",
    "    \"dow_jones_trading_volume\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "supply = load_pickle(\"dow_jones_aggregated_supply_data\")\n",
    "trading_volume = load_pickle(\"dow_jones_trading_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [supply, trading_volume],\n",
    "    axis=\"columns\",\n",
    "    join=\"outer\"\n",
    ")\n",
    "df = df.rename(columns={0:\"trading_volume\"})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df[\"index\"] = \"'\" + df[\"index\"].astype(\"str\") + \"'\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_insert_into_db(\n",
    "    CONNECTION_STRING,\n",
    "    \"dow_jones_trading_metadata\",\n",
    "    df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_into_db(\n",
    "    CONNECTION_STRING,\n",
    "    \"dow_jones_trading_metadata\",\n",
    "    df.to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "# Gold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_supply = pd.read_excel(\n",
    "    \"../../temp/GDT_Tables_Q425_EN.xlsx\",\n",
    "    sheet_name=\"Gold Balance\"\n",
    ").iloc[4,22:-3]\n",
    "total_supply = total_supply.astype(float)\n",
    "total_supply.index = pd.date_range(start=\"2010-01-01\", freq=\"QE\", periods=63)\n",
    "total_supply = total_supply * 1000\n",
    "total_supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(\n",
    "    total_supply,\n",
    "    \"gold_aggregated_supply\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "## Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CMEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "* Downloaded data manually\n",
    "* Starts on 2013-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_exists = pd.Series(\n",
    "    False, \n",
    "    index=pd.date_range(start=\"2013-01-01\", end=\"2025-08-01\", freq=\"ME\").strftime(\"%Y%m\") + \".zip\"\n",
    ")\n",
    "file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in file_exists.index.copy():\n",
    "    file_exists.loc[i] = Path(f\"../../temp/{i}\").exists()\n",
    "file_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_exists[file_exists == False]\n",
    "# all files are present"
   ]
  },
  {
   "cell_type": "raw",
   "id": "116",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# should only be ran only once, hence raw cell\n",
    "\n",
    "for file in Path(\"../../temp/\").iterdir():\n",
    "    # other files will fail on extraction\n",
    "    if not re.match(r\"^20.{4}\\.zip$\", file.name): # match \"20****.zip\" files\n",
    "        continue\n",
    "    \n",
    "    base_name = file.name.split(\".\")[0]\n",
    "    \n",
    "    with zipfile.ZipFile(file, \"r\") as archive:\n",
    "        try: \n",
    "            archive.extract(f\"Web_Volume_Report_CMEG_{base_name}.pdf\", path=file.parent)\n",
    "            file.unlink()\n",
    "            continue\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            archive.extract(f\"{base_name}/Web_Volume_Report_CMEG_{base_name}.pdf\", path=file.parent)\n",
    "            file.unlink()\n",
    "        except Exception as err:\n",
    "            print(f\"FATAL: While processing, file 'Web_Volume_Report_CMEG_{base_name}.pdf' has not been found, skipping\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "117",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "TARGET_DIR = Path(\"/Users/Misha/Documents/python_projects/data_analysis/bitcoin_analysis/temp/\")\n",
    "\n",
    "if not TARGET_DIR.is_dir():\n",
    "    raise ValueError(\"TARGET_DIR must be an existing directory\")\n",
    "\n",
    "for item in list(TARGET_DIR.iterdir()):\n",
    "    if not item.is_dir():\n",
    "        continue\n",
    "        \n",
    "    for sub_item in item.iterdir():\n",
    "        destination = TARGET_DIR / sub_item.name\n",
    "\n",
    "        if destination.exists():\n",
    "            raise FileExistsError(f\"Name collision: {destination} already exists\")\n",
    "\n",
    "        shutil.move(str(sub_item), str(destination))\n",
    "    item.rmdir()\n",
    "\n",
    "# some achives deviate, thus they cannot be unpacked with these two scripts (cell above and this). They thus require manual extraction. \n",
    "# This is especially true of 2015-01, where the report is mistakenly called ...201412.zip"
   ]
  },
  {
   "cell_type": "raw",
   "id": "118",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# this script should also be ran only once\n",
    "for i in list(Path(\"../../temp/\").iterdir()):\n",
    "    if not i.name.startswith(\"Web\"):\n",
    "        print(f\"Unrecognized object: {i.name}\")\n",
    "        continue\n",
    "    \n",
    "    shutil.move(i, i.parent / i.name[18:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_files = list(\n",
    "    \"CMEG_\" + pd.date_range(start=\"2013-01-01\", end=\"2025-08-01\", freq=\"ME\").strftime(\"%Y%m\") + \".pdf\"\n",
    ")\n",
    "expected_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore dot files\n",
    "got_files = list(i.name for i in Path(\"../../temp/\").iterdir() if not i.name.startswith(\".\"))\n",
    "got_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(expected_files) == len(got_files), f\"The number of actual files does not equal what is expected. Expected: {len(expected_files)}, got {len(got_files)}\"\n",
    "\n",
    "assert len(got_files) == len(set(got_files)), \"There are duplicated in fetched files\"\n",
    "\n",
    "for exp, got in zip(sorted(expected_files), sorted(got_files)):\n",
    "    assert exp == got, f\"File names don't match: {exp} vs. {got}\"\n",
    "else:\n",
    "    print(f\"All expected files have been fetched and processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = pd.Series()\n",
    "\n",
    "for file in Path(\"../../temp/\").iterdir():\n",
    "    if not file.name.startswith(\"CMEG\"):\n",
    "        continue\n",
    "\n",
    "    for name in [\"COMEX GOLD\"]:            \n",
    "        try:\n",
    "            volume = extract_asset_volume(\n",
    "                path=file,\n",
    "                asset=name\n",
    "            )\n",
    "            break\n",
    "        except ValueError:\n",
    "            volume = -1\n",
    "    srs.loc[datetime(int(file.name[5:9]), int(file.name[9:11]), 1)] = volume\n",
    "    print(f\"{datetime(int(file.name[5:9]), int(file.name[9:11]), 1)}: {volume}\")\n",
    "\n",
    "srs = srs.sort_index()\n",
    "srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual sanity check for year 2022\n",
    "# since there are multiple COMEX GOLD entries. Luckily we only need the first one\n",
    "srs[srs.index.year==2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert units\n",
    "srs = (1 / 0.311034768) * srs\n",
    "srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(srs, \"CMEG_gold_volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Shanghai exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.shfe.com.cn/eng/reports/StatisticalData/MonthlyData/?query_params=month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def wait_dom_stable(driver, *, min_wait=1, stable_for=1, timeout=15.0, poll=0.1):\n",
    "    \"\"\"\n",
    "    Wait until driver.page_source stops changing for `stable_for` seconds,\n",
    "    but never return before `min_wait` seconds have elapsed.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    end = start + timeout\n",
    "\n",
    "    last = driver.page_source\n",
    "    last_change = start\n",
    "\n",
    "    while True:\n",
    "        now = time.time()\n",
    "        if now >= end:\n",
    "            raise TimeoutError(\"DOM did not stabilize within timeout\")\n",
    "\n",
    "        time.sleep(poll)\n",
    "        cur = driver.page_source\n",
    "\n",
    "        if cur != last:\n",
    "            last = cur\n",
    "            last_change = time.time()\n",
    "\n",
    "        now = time.time()\n",
    "        # require both: (a) we've waited at least min_wait and (b) stable window reached\n",
    "        if (now - start) >= min_wait and (now - last_change) >= stable_for:\n",
    "            return\n",
    "\n",
    "xpath = (\n",
    "        \"//button\"\n",
    "        \"[contains(@class,'el-button') \"\n",
    "        \"and contains(@class,'el-button--plain') \"\n",
    "        \"and contains(@class,'el-button--mini') \"\n",
    "        \"and not(@disabled) \"\n",
    "        \"and not(contains(@class,'is-disabled')) \"\n",
    "        \"and (.//*[normalize-space()='‹' or contains(normalize-space(),'‹')] \"\n",
    "        \"     or contains(normalize-space(),'‹'))]\"\n",
    "    )\n",
    "wait = WebDriverWait(driver, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(187):\n",
    "    wait_dom_stable(driver)\n",
    "    sleep(random.uniform(1,2))\n",
    "    \n",
    "    try:\n",
    "        tables = pd.read_html(StringIO(driver.page_source))\n",
    "    except Exception:\n",
    "        tables = \"Error\"\n",
    "    \n",
    "    try:\n",
    "        month = driver.find_element(By.CLASS_NAME, \"el-calendar__title\").get_attribute(\"textContent\").strip()\n",
    "    except Exception:\n",
    "        month = \"Error\"\n",
    "\n",
    "    data.append(\n",
    "        (month, tables)\n",
    "    )\n",
    "\n",
    "    # Skip to the next month\n",
    "    \n",
    "    # wait for at least one matching button\n",
    "    wait.until(lambda d: len(d.find_elements(By.XPATH, xpath)) > 0)\n",
    "    \n",
    "    # choose the first one that is actually displayed + enabled\n",
    "    btn = next(\n",
    "        (b for b in driver.find_elements(By.XPATH, xpath)\n",
    "         if b.is_displayed() and b.is_enabled()),\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    # bring into view (centered tends to avoid sticky overlays)\n",
    "    driver.execute_script(\n",
    "        \"arguments[0].scrollIntoView({block:'center', inline:'center'});\", btn\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        btn.click()\n",
    "    except ElementClickInterceptedException:\n",
    "        # common when an overlay intercepts the click; try moving to it then clicking\n",
    "        ActionChains(driver).move_to_element(btn).pause(0.1).click().perform()\n",
    "    except AttributeError:\n",
    "        print(\"Cannot continue iteration. Button was not found\")\n",
    "        input(\"Enter to continue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(\n",
    "    data,\n",
    "    \"raw_scraped_schanghai_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_pickle(\"raw_scraped_schanghai_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "srs = pd.Series(\n",
    "    index=pd.date_range(start=\"2010-01-01\", end=\"2025-07-31\", freq=\"MS\")\n",
    ")\n",
    "\n",
    "problematic_entries = []\n",
    "\n",
    "for (_, i) in data:\n",
    "    date = datetime.fromisoformat(i[3].columns[0].split()[1] + \"-01\")\n",
    "    try:\n",
    "        volume = i[24].iloc[-1,9]\n",
    "    except Exception as err:\n",
    "        if \"index 9 is out of bounds for axis 0 with size 6\" == str(err):\n",
    "            #print(str(err))\n",
    "            volume = i[14].iloc[-1,9]\n",
    "        elif \"list index out of range\" == str(err):\n",
    "            volume = i[12].iloc[-1, 9]\n",
    "        else:\n",
    "            print(f\"{type(err)} - {err}\")\n",
    "            problematic_entries.append((date, i))\n",
    "            volume = np.nan\n",
    "    \n",
    "    srs.loc[date] = volume    \n",
    "\n",
    "srs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "srs[srs.isna()].sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(srs, \"shanghai_gold_volume\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "### TOCOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel(\"../../temp/CDF_M_202007.xlsx\").iloc[5:11,-8].sum()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
